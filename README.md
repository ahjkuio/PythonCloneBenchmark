# PythonCloneBenchmark

**PythonCloneBenchmark** — это проект по созданию эталонного набора данных (бенчмарка) для детектирования клонов в коде на языке Python. Проект вдохновлен существующими бенчмарками для Java, такими как BigCloneBench, и использует в качестве источника данных решения задач из Google Code Jam (GCJ).

## Цель проекта

Основная цель — предоставить сообществу исследователей и разработчиков качественный, открытый датасет с известными парами клонов Python-кода, который может быть использован для обучения, тестирования и сравнения различных инструментов и методов обнаружения клонов.
Кроме того, проект включает инструменты для оценки качества работы детекторов клонов на основе этого бенчмарка.

## Текущий статус

*   **[Выполнено]** Инициализирован Git-репозиторий проекта.
*   **[Выполнено]** Определена базовая структура директорий.
*   **[Выполнено]** Разработан скрипт (`scripts/setup_project.py`) для:
    *   Автоматического скачивания и распаковки годовых CSV-файлов Google Code Jam (из `Jur1cek/gcj-dataset`).
    *   Создания необходимой структуры директорий проекта.
    *   (Опционально) Установки Python-зависимостей из `requirements.txt`.
*   **[Выполнено]** Разработан скрипт (`scripts/build_benchmark.py`) для:
    *   Чтения годовых CSV-файлов Google Code Jam (предварительно скачанных с помощью `setup_project.py`).
    *   Извлечения решений на языке Python.
    *   Подсчета количества строк в каждом решении.
    *   Сохранения извлеченных Python-решений в отдельные `.py` файлы (пути относительные от корня проекта).
    *   Генерации CSV-файла (`benchmark_output/clones_ГОД.csv`) с парами клонов (решения одной задачи GCJ, пути к файлам относительные).
*   **[Выполнено]** Разработан скрипт (`scripts/load_tool_results_to_db.py`) для загрузки результатов работы произвольного детектора клонов (в формате CSV) в базу данных SQLite.
*   **[Выполнено]** Разработан скрипт (`scripts/evaluate_clones.py`) для оценки качества работы детектора клонов путем сравнения его результатов (из БД SQLite) с эталонным бенчмарком. Расчет метрик: TP, FP, FN, Precision, Recall, F1-score.
*   **[Выполнено]** Разработан скрипт (`scripts/generate_pseudo_real_detector_output.py`) для генерации имитированных (псевдо-реальных) результатов детектора на основе эталонного бенчмарка, для тестирования системы оценки.
*   **[Выполнено]** Тестирование и отладка процесса оценки с использованием имитированных результатов детектора (`smart_mock_detector_results.csv`), подтверждена корректность расчета метрик TP, FP, FN, Precision, Recall, F1-score на основе c-match.

## Структура проекта

```
PythonCloneBenchmark/
├── .git/                       # Git-репозиторий
├── .gitignore                  # Файл для исключения ненужных файлов из Git
├── data/                       # Директория для исходных данных
│   ├── gcj_csv_archives/       # Скачанные архивы GCJ CSV (например, gcj2017.csv.tar.bz2)
│   ├── gcj_csv_unpacked/       # Распакованные GCJ CSV (например, gcj2017.csv)
│   ├── mock_detector_output/   # Примеры CSV-файлов с результатами работы "детекторов"
│   │   ├── mock_detector_results.csv
│   │   ├── smart_mock_detector_results.csv
│   │   └── pseudo_real_results_Y2017_T70.csv # Пример псевдо-реальных результатов
│   └── tool_results/           # Базы данных SQLite с загруженными результатами детекторов
│       └── tool_results.db
├── extracted_solutions/        # Директория для извлеченных .py файлов решений
│   └── ГОД/
│       └── TASK_ID/
│           └── USERNAME/
│               └── FILENAME.py
├── benchmark_output/           # Директория для сгенерированных CSV-файлов с парами клонов
│   └── clones_ГОД.csv
├── scripts/                    # Директория со скриптами
│   ├── setup_project.py        # Скрипт для первоначальной настройки проекта (скачивание данных, создание папок)
│   ├── build_benchmark.py      # Основной скрипт для сборки бенчмарка
│   ├── load_tool_results_to_db.py # Скрипт для загрузки результатов детектора в БД
│   ├── evaluate_clones.py      # Скрипт для оценки результатов детектора
│   └── generate_pseudo_real_detector_output.py # Скрипт для генерации псевдо-реальных результатов
├── docs/                       # (Пока не используется) Директория для дополнительной документации
├── requirements.txt            # Файл с Python-зависимостями
└── README.md                   # Этот файл
```

## Как это работает

Процесс работы с PythonCloneBenchmark можно разделить на два основных этапа: создание эталонного бенчмарка и оценка детектора клонов.

### 1. Создание эталонного бенчмарка

1.  **Первоначальная настройка (`setup_project.py`)**:
    *   Скачивает и распаковывает необходимые годовые CSV-файлы Google Code Jam.
    *   Создает всю необходимую структуру директорий.
    *   (Опционально) Устанавливает Python-зависимости.
2.  **Запуск `build_benchmark.py`**: Этот скрипт читает CSV-файл GCJ (скачанный на предыдущем шаге), фильтрует Python-решения, сохраняет их в структурированные директории в `extracted_solutions/` (с относительными путями от корня проекта) и генерирует файл `benchmark_output/clones_ГОД.csv`. В этом файле каждая строка представляет собой пару Python-решений, являющихся решениями одной и той же задачи GCJ, и, следовательно, считающихся клонами. Координаты клонов указываются как полный файл (начало строки 0, конец – последняя строка).

### 2. Оценка детектора клонов

1.  **Подготовка результатов детектора**: Результаты работы вашего инструмента для обнаружения клонов должны быть представлены в виде CSV-файла. Каждая строка должна представлять одну обнаруженную пару клонов и содержать как минимум следующие колонки (с точными такими именами):
    *   `file1_path`: Абсолютный или относительный (относительно корня проекта `PythonCloneBenchmark/`) путь к первому файлу в паре.
    *   `file1_start`: Начальная строка клонированного фрагмента в первом файле (0-индексация).
    *   `file1_end`: Конечная строка клонированного фрагмента в первом файле (0-индексация, включительно).
    *   `file2_path`: Абсолютный или относительный (относительно корня проекта `PythonCloneBenchmark/`) путь ко второму файлу в паре.
    *   `file2_start`: Начальная строка клонированного фрагмента во втором файле (0-индексация).
    *   `file2_end`: Конечная строка клонированного фрагмента во втором файле (0-индексация, включительно).
    Пример такого файла можно найти в `data/mock_detector_output/`.
    Важно: Скрипт оценки (`evaluate_clones.py`) ожидает, что пути к файлам в вашем CSV (и, соответственно, в БД) будут либо абсолютными, либо относительными от корня проекта `PythonCloneBenchmark/`. В процессе оценки все пути преобразуются к абсолютным каноническим путям для сравнения. Убедитесь, что ваши пути указывают на файлы внутри директории `extracted_solutions/ГОД/TASK_ID/USERNAME/FILENAME.py`.

2.  **Загрузка результатов детектора в БД (`load_tool_results_to_db.py`)**: Этот скрипт читает ваш CSV-файл с результатами и загружает их в таблицу базы данных SQLite (например, `data/tool_results/tool_results.db`). Это делается для более эффективного доступа при оценке.

3.  **Запуск оценки (`evaluate_clones.py`)**: Этот скрипт сравнивает клоны, обнаруженные вашим инструментом (из БД SQLite), с эталонными клонами из `benchmark_output/clones_ГОД.csv`. На основе этого сравнения рассчитываются метрики качества.

## Оценка качества обнаружения клонов

### Цель оценки

Оценка позволяет количественно измерить, насколько хорошо инструмент для обнаружения клонов справляется со своей задачей по сравнению с известным набором истинных клонов (эталонным бенчмарком). Она отвечает на вопросы: "Насколько точен детектор?" и "Много ли клонов он пропускает?".

### Методология сравнения: `c-match`

Простое посимвольное сравнение кода, найденного детектором, с кодом из эталона было бы неэффективным. Детектор может найти клон, являющийся лишь частью эталонного файла, или эталонный файл может быть частью большего клона, найденного детектором. Также могут присутствовать незначительные различия (форматирование, комментарии, имена переменных), не влияющие на суть клонирования.

Поэтому для определения, соответствует ли пара клонов, обнаруженная инструментом, паре клонов из эталонного бенчмарка, используется метрика **`c-match` (Coverage Match)**, предложенная в работе *Svajlenko & Roy, "Evaluating Clone Detection Tools with BigCloneBench" (ICSME 2015)*.

Процесс `c-match` для одной эталонной пары (`Эталон_Файл1`, `Эталон_Файл2`) и одной обнаруженной детектором пары (`Детектор_Фрагмент1`, `Детектор_Фрагмент2`):

1.  **Сопоставление файлов**:
    *   Сначала проверяется, относятся ли фрагменты к одним и тем же исходным файлам. Пути к файлам из эталонной пары должны совпадать с путями к файлам из обнаруженной пары. Допускается два варианта:
        *   Прямое совпадение: `Эталон_Файл1.path == Детектор_Фрагмент1.path` И `Эталон_Файл2.path == Детектор_Фрагмент2.path`
        *   Обратное совпадение: `Эталон_Файл1.path == Детектор_Фрагмент2.path` И `Эталон_Файл2.path == Детектор_Фрагмент1.path`
    *   Если пути не совпадают ни одним из этих способов, пара не считается совпадением (`c-match = False`), и дальнейшая проверка для этой пары не производится.

2.  **Расчет покрытия строк (Line Coverage)**: Если файлы успешно сопоставлены (например, `Эталон_Файл1` с `Детектор_Фрагмент1`, и `Эталон_Файл2` с `Детектор_Фрагмент2`), для каждой такой пары фрагментов рассчитывается взаимное покрытие по строкам.
    *   `Эталон_ФайлN` в нашем случае — это строки с 0 до конца файла. `Детектор_ФрагментN` — это строки, указанные детектором (например, с 10 по 25).
    *   Рассчитывается четыре значения покрытия:
        *   `coverage(Эталон_Файл1, Детектор_Фрагмент1) = (кол-во общих строк) / (кол-во строк в Детектор_Фрагмент1)`
        *   `coverage(Детектор_Фрагмент1, Эталон_Файл1) = (кол-во общих строк) / (кол-во строк в Эталон_Файл1)`
        *   `coverage(Эталон_Файл2, Детектор_Фрагмент2) = (кол-во общих строк) / (кол-во строк в Детектор_Фрагмент2)`
        *   `coverage(Детектор_Фрагмент2, Эталон_Файл2) = (кол-во общих строк) / (кол-во строк в Эталон_Файл2)`
    *   Это означает, что мы проверяем, насколько хорошо эталонный фрагмент покрывает найденный детектором, и наоборот.

3.  **Проверка порогового значения (Threshold)**: Используется пороговое значение покрытия (по умолчанию `0.7`, т.е. 70%). Пара считается совпавшей (`c-match = True`) **только если все четыре** вышеуказанных условия покрытия выполняются (т.е. каждое из четырех рассчитанных значений покрытия `>= threshold`).
    *   Если хотя бы одно из этих условий не выполняется, то `c-match = False` для данной пары эталона и детектора.

### Оптимизация сопоставления

Для ускорения процесса оценки, скрипт `evaluate_clones.py` использует `task_id` (идентификатор задачи из Google Code Jam). При поиске соответствия для эталонного клона, сначала отбираются только те обнаруженные клоны, у которых `task_id` (извлеченный из пути к файлу) совпадает с `task_id` эталонного клона. Это значительно сокращает количество пар для проверки `c-match`.

### Метрики качества

На основе результатов `c-match` рассчитываются стандартные метрики для оценки качества работы детектора:

*   **True Positives (TP) - Истинно Положительные**:
    *   Количество пар клонов из вашего **эталонного бенчмарка**, для которых детектор **нашел соответствующую пару**, удовлетворяющую условию `c-match`.
    *   *Простыми словами*: Детектор правильно определил клон, который действительно существует в эталоне.

*   **False Positives (FP) - Ложно Положительные**:
    *   Количество пар, **обнаруженных детектором**, для которых **не нашлось** соответствующей пары в вашем **эталонном бенчмарке** (или нашлось, но пара не прошла проверку `c-match`).
    *   *Простыми словами*: Детектор сообщил "это клон!", но на самом деле (согласно вашему эталону) это не клон, или он настолько не похож на эталонный, что не прошел проверку `c-match`. Это "ложная тревога" от детектора.

*   **False Negatives (FN) - Ложно Отрицательные**:
    *   Количество пар клонов из вашего **эталонного бенчмарка**, для которых детектор **не нашел** соответствующую пару (или найденная пара не прошла `c-match`).
    *   *Простыми словами*: В вашем эталоне есть реальный клон, но детектор его *пропустил*.

На основе этих трех базовых значений (TP, FP, FN) рассчитываются более высокоуровневые метрики:

*   **Precision (Точность)**:
    *   Формула: `TP / (TP + FP)`
    *   *Что показывает*: Какая доля из пар, которые детектор назвал клонами, *действительно* является клонами (согласно вашему эталону).
    *   *Пример*: Если Precision = 0.8 (или 80%), это означает, что 80% из того, что детектор идентифицировал как клоны, являются настоящими клонами, а остальные 20% — это ошибки (FP). Высокая точность важна, когда вы хотите быть уверены, что если детектор что-то нашел, то этому можно доверять.

*   **Recall (Полнота)**:
    *   Формула: `TP / (TP + FN)`
    *   *Что показывает*: Какую долю из *всех реально существующих* в вашем эталонном бенчмарке клонов детектор смог успешно обнаружить.
    *   *Пример*: Если Recall = 0.6 (или 60%), это означает, что детектор нашел 60% всех истинных клонов, но 40% пропустил (FN). Высокая полнота важна, когда главная цель — найти как можно больше реальных клонов.

*   **F1-Score (F-мера)**:
    *   Формула: `2 * (Precision * Recall) / (Precision + Recall)`
    *   *Что показывает*: Гармоническое среднее между точностью (Precision) и полнотой (Recall). Эта метрика полезна как единый показатель общего качества детектора, особенно когда важен баланс между точностью и полнотой.
    *   *Зачем нужна*: Часто улучшение одной метрики (например, Recall путем снижения порога чувствительности детектора) может привести к ухудшению другой (Precision, так как детектор начнет сообщать больше ложных срабатываний). F1-score помогает найти "золотую середину" и комплексно оценить детектор. Значение F1-score ближе к 1 указывает на лучшее качество.

## Как запустить

Этот раздел описывает шаги для полной настройки и использования PythonCloneBenchmark. Существует два основных сценария работы с бенчмарком:

1.  **Оценка вашего собственного детектора клонов**:
    *   Сначала вы настраиваете проект и собираете эталонный бенчмарк (наши "истинные" клоны).
    *   Затем вы подготавливаете CSV-файл с результатами вашего детектора.
    *   Загружаете результаты вашего детектора в базу данных.
    *   Запускаете скрипт оценки для сравнения ваших результатов с эталоном.
2.  **Тестирование системы оценки или просмотр примера работы (без собственного детектора)**:
    *   Сначала вы настраиваете проект и собираете эталонный бенчмарк.
    *   Затем вы генерируете "псевдо-реальные" результаты с помощью специального скрипта (он имитирует работу детектора).
    *   Загружаете эти псевдо-реальные результаты в базу данных.
    *   Запускаете скрипт оценки.

**Важно:** Эталонные файлы бенчмарка (например, `clones_ГОД.csv` в директории `benchmark_output/`) не хранятся в данном Git-репозитории. Их необходимо сгенерировать локально с помощью скрипта `build_benchmark.py` (см. Шаг 3) после первоначальной настройки проекта.

**Пошаговая инструкция:**

1.  **Клонируйте репозиторий** (если еще не сделали):
    ```bash
    git clone https://github.com/ahjkuio/PythonCloneBenchmark.git
    cd PythonCloneBenchmark
    ```

2.  **Первоначальная настройка проекта (`setup_project.py`)**:
    *   Этот скрипт скачает данные Google Code Jam для нужного года (например, 2017), создаст все необходимые директории и установит зависимости.
    *   Перейдите в директорию `scripts/` (если вы еще не там) и выполните:
        ```bash
        cd scripts
        python setup_project.py --year 2017 
        ```
        *   Вы можете указать другой год или несколько лет через запятую: `--year 2016,2017`.
        *   Используйте `--year all` для скачивания данных за 2008-2017 гг.
        *   Если вы хотите пропустить установку зависимостей (например, вы управляете ими вручную или используете виртуальное окружение, где они уже есть), добавьте флаг `--skip_dependencies`.
        *   Если данные GCJ уже скачаны, можно пропустить их повторное скачивание флагом `--skip_gcj_download`.
    *   Если установка зависимостей была пропущена, но они не установлены, установите их вручную:
        ```bash
        # Находясь в корневой директории PythonCloneBenchmark/
        pip install -r requirements.txt 
        ```
    *   После выполнения скрипта, CSV-файл (например, `gcj2017.csv`) должен находиться в `data/gcj_csv_unpacked/`.

3.  **Сборка эталонного бенчмарка (`build_benchmark.py`)**:
    *   Этот скрипт обработает скачанный GCJ CSV-файл, извлечет Python-решения и создаст эталонный CSV-файл с парами клонов.
    *   Находясь в директории `scripts/`, выполните команду:
        ```bash
        python build_benchmark.py --year 2017
        ```
        *   Замените `ГОД` на нужный год (например, `2017`).
        *   Скрипт ожидает, что соответствующий `gcjГОД.csv` уже находится в `../data/gcj_csv_unpacked/` (скачан скриптом `setup_project.py`).
        *   Вы можете явно указать путь к GCJ CSV файлу с помощью `--input_csv_path ../ПУТЬ/К/ВАШЕМУ/gcjГОД.csv`.
        *   Также можно переопределить директории для извлеченных решений и эталонного бенчмарка с помощью `--extracted_solutions_dir` и `--benchmark_output_dir` (указываются относительно корня проекта).
    *   Скрипт создаст/обновит файлы в директориях `../extracted_solutions/` и `../benchmark_output/`. В частности, будет создан `../benchmark_output/clones_2017.csv`.

4.  **Подготовка и загрузка результатов вашего детектора (Сценарий 1) ИЛИ Генерация псевдо-реальных результатов (Сценарий 2)**:

    *   **Сценарий 1: У вас есть свой детектор**
        *   Подготовьте CSV-файл с результатами вашего детектора (см. формат в разделе "Оценка качества -> Подготовка результатов детектора"). Пути к файлам должны быть либо абсолютными, либо относительными от корня `PythonCloneBenchmark/` и указывать на файлы в `extracted_solutions/`.
        *   Поместите его, например, в `../data/mock_detector_output/my_tool_results.csv`.
        *   Запустите скрипт `load_tool_results_to_db.py` (из директории `scripts/`):
            ```bash
            python load_tool_results_to_db.py --csv_file ../data/mock_detector_output/my_tool_results.csv --db_file ../data/tool_results/tool_results.db
            ```
    *   **Сценарий 2: Тестирование системы оценки (генерация псевдо-результатов)**
        *   Если у вас нет своего детектора, но вы хотите протестировать процесс оценки, используйте `generate_pseudo_real_detector_output.py`.
        *   Находясь в директории `scripts/`, выполните:
            ```bash
            python generate_pseudo_real_detector_output.py --benchmark_csv ../benchmark_output/clones_2017.csv --output_csv ../data/mock_detector_output/pseudo_real_results_Y2017_T70.csv --similarity_threshold 0.70
            ```
            *   `--benchmark_csv`: Укажите путь к эталонному бенчмарку, созданному на шаге 3.
            *   `--output_csv`: Путь для сохранения сгенерированного CSV.
            *   `--similarity_threshold`: Порог "схожести" строк (от 0.0 до 1.0), который будет использоваться для имитации обнаружения клона. Чем выше порог, тем меньше "клонов" найдет этот скрипт.
        *   Затем загрузите эти сгенерированные результаты в БД:
            ```bash
            python load_tool_results_to_db.py --csv_file ../data/mock_detector_output/pseudo_real_results_Y2017_T70.csv --db_file ../data/tool_results/tool_results.db
            ```
    Это создаст (или перезапишет) базу данных `tool_results.db` с данными для оценки.

5.  **Запуск оценки (`evaluate_clones.py`)**:
    *   Выполните команду (из директории `scripts/`):
        ```bash
        python evaluate_clones.py --benchmark_csv ../benchmark_output/clones_2017.csv --tool_db ../data/tool_results/tool_results.db
        ```
        *   Замените `clones_2017.csv` на актуальное имя вашего эталонного файла, если оно отличается.
    *   Скрипт выведет рассчитанные метрики (TP, FP, FN, Precision, Recall, F1-score).
    *   **Опциональные параметры для `evaluate_clones.py`**:
        *   `--threshold FLOAT`: Порог покрытия для `c-match` (по умолчанию `0.7`).
        *   `--tool_table_name TEXT`: Имя таблицы в БД с результатами детектора (по умолчанию `detected_clones`).

## Дальнейшие шаги

*   Изучение и реализация других метрик для оценки качества обнаружения клонов (например, `sc-match`, `fc-match` из Svajlenko ICSME 2015).
*   Потенциальное уточнение границ клонов в эталонном бенчмарке (например, до уровня функций, а не целых файлов).
*   Расширение бенчмарка данными за другие годы.
